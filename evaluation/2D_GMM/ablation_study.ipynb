{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from tbparse import SummaryReader\n",
    "import matplotlib\n",
    "\n",
    "from utils import (\n",
    "    get_validation_loader_dict_2D_GMM,\n",
    "    load_INN,\n",
    "    p_beta\n",
    ")\n",
    "\n",
    "from pinf.models.GMM import GMM\n",
    "from pinf.datasets.parameters import (\n",
    "    means_2D_GMM,\n",
    "    S_2D_GMM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the validation data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_list = torch.linspace(np.log(0.1),np.log(10),20).exp()\n",
    "T_list = torch.cat((T_list,torch.tensor([1.0])))\n",
    "T_list = [round(T_list.sort().values[i].item(),5) for i in range(len(T_list))]\n",
    "\n",
    "a = 3\n",
    "T_list_eval = T_list[10 - a:-(10 - a)]\n",
    "\n",
    "validation_data_loader_dict = get_validation_loader_dict_2D_GMM(T_list_eval = T_list_eval,n_samples = 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the paths to the trained models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_MODE = \"proposal_distribution_evaluation_points\"\n",
    "\n",
    "if EXPERIMENT_MODE == \"proposal_distribution_evaluation_points\":\n",
    "\n",
    "    path_dict = {\n",
    "        \"grid_model\":                       \"../../results/runs_2D_GMM/<Your experiment name>/lightning_logs/version_0/\",\n",
    "        \"grid_training_data\":               \"../../results/runs_2D_GMM/<Your experiment name>/lightning_logs/version_0/\",\n",
    "        \"grid_model_at_base_parameter\":     \"../../results/runs_2D_GMM/<Your experiment name>/lightning_logs/version_0/\",\n",
    "        \"grid_standard_normal\":             \"../../results/runs_2D_GMM/<Your experiment name>/lightning_logs/version_0/\",\n",
    "        \"no_grid_model\":                    \"../../results/runs_2D_GMM/<Your experiment name>/lightning_logs/version_0/\",\n",
    "        \"no_grid_model_at_base_parameter\":  \"../../results/runs_2D_GMM/<Your experiment name>/lightning_logs/version_0/\",\n",
    "        \"no_grid_standard_normal\":          \"../../results/runs_2D_GMM/<Your experiment name>/lightning_logs/version_0/\"\n",
    "    }   \n",
    "\n",
    "    INN_dict = {}\n",
    "    config_dict = {}\n",
    "\n",
    "    for key in path_dict:\n",
    "        INN_k,config_k = load_INN(base_path = path_dict[key],device=device,use_last=False)\n",
    "        INN_dict[key] = INN_k\n",
    "        config_dict[key] = config_k\n",
    "\n",
    "    a = 7\n",
    "    T_list_eval = T_list[10 - a:-(10 - a)]\n",
    "\n",
    "    validation_data_loader_dict = get_validation_loader_dict_2D_GMM(T_list_eval = T_list_eval,n_samples = 100000)\n",
    "\n",
    "elif EXPERIMENT_MODE == \"grid_loss_balancing\":\n",
    "    base_path = \"../../results/runs_2D_GMM/<Your experiment name>/lightning_logs/\"\n",
    "\n",
    "    subfolders = os.listdir(base_path)\n",
    "\n",
    "    INN_dict = {}\n",
    "    config_dict = {}\n",
    "\n",
    "    for i in range(len(subfolders)):\n",
    "\n",
    "        folder_i = os.path.join(base_path,subfolders[i])\n",
    "\n",
    "        INN_i,config_i = load_INN(base_path = folder_i,device=device,use_last=False)\n",
    "\n",
    "        config_dict[subfolders[i]] = config_i\n",
    "        INN_dict[subfolders[i]] = INN_i\n",
    "\n",
    "elif EXPERIMENT_MODE == \"no_grid_loss_balancing\":\n",
    "    base_path = \"../../results/runs_2D_GMM/<Your experiment name>/lightning_logs/\"\n",
    "\n",
    "    subfolders = os.listdir(base_path)\n",
    "\n",
    "    INN_dict = {}\n",
    "    config_dict = {}\n",
    "\n",
    "    for i in range(len(subfolders)):\n",
    "\n",
    "        folder_i = os.path.join(base_path,subfolders[i])\n",
    "\n",
    "        INN_i,config_i = load_INN(base_path = folder_i,device=device,use_last=False)\n",
    "\n",
    "        config_dict[subfolders[i]] = config_i\n",
    "        INN_dict[subfolders[i]] = INN_i\n",
    "\n",
    "elif EXPERIMENT_MODE == \"no_grid_number_evaluation_points_vs_evaluation_parms\":\n",
    "\n",
    "    base_path = \"../../results/runs_2D_GMM/<Your experiment name>/lightning_logs/\"\n",
    "\n",
    "    subfolders = os.listdir(base_path)\n",
    "\n",
    "    INN_dict = {}\n",
    "    config_dict = {}\n",
    "\n",
    "    for i in range(len(subfolders)):\n",
    "\n",
    "        folder_i = os.path.join(base_path,subfolders[i])\n",
    "\n",
    "        INN_i,config_i = load_INN(base_path = folder_i,device=device,use_last=False)\n",
    "\n",
    "        config_dict[subfolders[i]] = config_i\n",
    "        INN_dict[subfolders[i]] = INN_i\n",
    "\n",
    "elif EXPERIMENT_MODE == \"no_grid_number_samples_expectation_approx\":\n",
    "\n",
    "    base_path = \"../../results/runs_2D_GMM/<Your experiment name>/lightning_logs/\"\n",
    "\n",
    "    subfolders = os.listdir(base_path)\n",
    "\n",
    "    INN_dict = {}\n",
    "    config_dict = {}\n",
    "\n",
    "    for i in range(len(subfolders)):\n",
    "\n",
    "        folder_i = os.path.join(base_path,subfolders[i])\n",
    "\n",
    "        INN_i,config_i = load_INN(base_path = folder_i,device=device,use_last=False)\n",
    "\n",
    "        config_dict[subfolders[i]] = config_i\n",
    "        INN_dict[subfolders[i]] = INN_i\n",
    "\n",
    "elif EXPERIMENT_MODE == \"grid_number_samples_expectation_approx\":\n",
    "\n",
    "    base_path = \"../../results/runs_2D_GMM/<Your experiment name>/lightning_logs/\"\n",
    "\n",
    "    subfolders = os.listdir(base_path)\n",
    "\n",
    "    INN_dict = {}\n",
    "    config_dict = {}\n",
    "\n",
    "    for i in range(len(subfolders)):\n",
    "\n",
    "        folder_i = os.path.join(base_path,subfolders[i])\n",
    "\n",
    "        INN_i,config_i = load_INN(base_path = folder_i,device=device,use_last=False)\n",
    "\n",
    "        config_dict[subfolders[i]] = config_i\n",
    "        INN_dict[subfolders[i]] = INN_i\n",
    "\n",
    "elif EXPERIMENT_MODE == \"grid_number_grid_points\":\n",
    "\n",
    "    base_path = \"../../results/runs_2D_GMM/<Your experiment name>/lightning_logs/\"\n",
    "\n",
    "    subfolders = os.listdir(base_path)\n",
    "\n",
    "    INN_dict = {}\n",
    "    config_dict = {}\n",
    "\n",
    "    for i in range(len(subfolders)):\n",
    "\n",
    "        folder_i = os.path.join(base_path,subfolders[i])\n",
    "\n",
    "        INN_i,config_i = load_INN(base_path = folder_i,device=device,use_last=False)\n",
    "\n",
    "        config_dict[subfolders[i]] = config_i\n",
    "        INN_dict[subfolders[i]] = INN_i\n",
    "\n",
    "elif EXPERIMENT_MODE == \"grid_epsilon_causality_weight\":\n",
    "\n",
    "    base_path = \"../../results/runs_2D_GMM/<Your experiment name>/lightning_logs/\"\n",
    "\n",
    "    subfolders = os.listdir(base_path)\n",
    "\n",
    "    INN_dict = {}\n",
    "    config_dict = {}\n",
    "\n",
    "    for i in range(len(subfolders)):\n",
    "\n",
    "        folder_i = os.path.join(base_path,subfolders[i])\n",
    "\n",
    "        INN_i,config_i = load_INN(base_path = folder_i,device=device,use_last=False)\n",
    "\n",
    "        config_dict[subfolders[i]] = config_i\n",
    "        INN_dict[subfolders[i]] = INN_i\n",
    "\n",
    "elif EXPERIMENT_MODE == \"learned_energy\":\n",
    "        path_dict = {\n",
    "            \"no_grid_gt_energy\":            \"../../results/runs_2D_GMM/<Your experiment name>/lightning_logs/version_0/\",,\n",
    "            \"no_grid_learned_energy\":       \"../../results/runs_2D_GMM/<Your experiment name>/lightning_logs/version_0/\",,\n",
    "            \"grid_gt_energy\":               \"../../results/runs_2D_GMM/<Your experiment name>/lightning_logs/version_0/\",,\n",
    "            \"grid_learned_energy\":          \"../../results/runs_2D_GMM/<Your experiment name>/lightning_logs/version_0/\",,\n",
    "            \"volume_preserving\":            \"../../results/runs_2D_GMM/<Your experiment name>/lightning_logs/version_0/\",\n",
    "        }   \n",
    "\n",
    "        INN_dict = {}\n",
    "        config_dict = {}\n",
    "\n",
    "        for key in path_dict:\n",
    "            INN_k,config_k = load_INN(base_path = path_dict[key],device=device,use_last=False)\n",
    "            INN_dict[key] = INN_k\n",
    "            config_dict[key] = config_k\n",
    "            \n",
    "results_folder = f\"./results_{EXPERIMENT_MODE}\"\n",
    "\n",
    "if not os.path.exists(results_folder):\n",
    "    os.mkdir(results_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_run(folder:str)->bool:\n",
    "    \"\"\"\n",
    "    Check if the experiment ran till the end or if it terminated prematurely.\n",
    "\n",
    "    parameters:\n",
    "        folder:     Location of the experiment to examine.\n",
    "\n",
    "    returns:\n",
    "        flag:       Indicator whether or not the run ran till the end.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    reader_k = SummaryReader(folder)\n",
    "    df_k = reader_k.scalars\n",
    "    df_red = df_k[(df_k[\"tag\"] == \"model_performance/mean_validation_KL\")]\n",
    "\n",
    "    steps_y = df_red[\"step\"].values\n",
    "    y_k = steps_y\n",
    "\n",
    "    epochs_ran = np.ceil(y_k.max() / config_dict[key][\"config_training\"][\"n_batches_per_epoch\"])\n",
    "\n",
    "    flag = (epochs_ran == config_dict[key][\"config_training\"][\"n_epochs\"])\n",
    "\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the ground truth data distribution\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GMM(means = means_2D_GMM,covs=S_2D_GMM,device=device)\n",
    "\n",
    "with open(\"../data/2D_GMM/Z_T.json\",\"r\") as f:\n",
    "    Z_T_dict = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation_functions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bootstrap_mean_and_error_KLD(log_p_gt:torch.tensor,log_p_theta:torch.tensor,n_bootstrap)->tuple:\n",
    "    \"\"\"\n",
    "    Examine the model perfromance:\n",
    "\n",
    "    parameters:\n",
    "        log_p_gt:       Log-likelihoods under the target distribution.\n",
    "        log_p_theta:    Log-likelihoods under the model distribution.\n",
    "        n_bootstrap:    Number of resampling steps for bootstrapping\n",
    "\n",
    "    returns:\n",
    "        mean_i,error_i: Mean and bootstrap standard error for the KL divergence\n",
    "    \"\"\"\n",
    "\n",
    "    samples = np.zeros(n_bootstrap)\n",
    "\n",
    "    for i in range(n_bootstrap):\n",
    "        indices = np.random.randint(0,len(log_p_gt),len(log_p_gt))\n",
    "        samples[i] = (log_p_gt[indices] - log_p_theta[indices]).mean()\n",
    "\n",
    "    mean_samples = samples.mean()\n",
    "    error_i = np.sqrt(np.square(samples - mean_samples).sum() / (n_bootstrap - 1))\n",
    "\n",
    "    mean_i = (log_p_gt - log_p_theta).mean().item()\n",
    "\n",
    "    return mean_i,error_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_KLs(INN_dict:dict,n_bootstrap:int = 20)->tuple:\n",
    "    \"\"\"\n",
    "    Evaluate multiple models at once.\n",
    "\n",
    "    parameters:\n",
    "        INN_dict:       Dictionary of INNs\n",
    "        n_bootstrap:    Number of resampling steps for bootstrapping\n",
    "\n",
    "    returns:\n",
    "        average_KL_dicts:       Average validation KL divergence for each model.\n",
    "        error_average_KL_dicts  Bootstrap error of the average validation KL divergence for each model.\n",
    "        errorKL_per_T_dicts     Validation KL divergence for each model and each evaluated temperature.\n",
    "        KL_per_T_dicts          Bootstrap error of the validation KL divergence for each model and each evaluated temperature.\n",
    "    \"\"\"\n",
    "\n",
    "    nll_dicts = {}\n",
    "    error_dicts = {}\n",
    "\n",
    "    KL_per_T_dicts = {}\n",
    "    errorKL_per_T_dicts = {}\n",
    "\n",
    "    for T_i in T_list_eval:\n",
    "        nll_dicts[f\"{T_i}\"] = {}\n",
    "        error_dicts[f\"{T_i}\"] = {}\n",
    "\n",
    "        KL_per_T_dicts[f\"{T_i}\"] = {}\n",
    "        errorKL_per_T_dicts[f\"{T_i}\"] = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Get the ground truth nlls\n",
    "        log_p_gt_dict = {}\n",
    "        \n",
    "        for T_i in T_list_eval:\n",
    "            T_i = round(T_i,5)\n",
    "\n",
    "            DL_i = validation_data_loader_dict[f\"{T_i}\"]\n",
    "\n",
    "            log_p_gt_val = torch.zeros([0])\n",
    "\n",
    "            for j,(beta_batch,x_batch) in enumerate(DL_i):\n",
    "\n",
    "                log_p_gt_val_i = p_beta(x_batch.to(device),beta = 1 / T_i,gmm = gmm,Z = Z_T_dict[f\"{T_i}\"]).log()\n",
    "                log_p_gt_val = torch.cat((log_p_gt_val,log_p_gt_val_i.detach().cpu()),0)\n",
    "\n",
    "            log_p_gt_dict[f\"{T_i}\"] = log_p_gt_val\n",
    "\n",
    "        # Get the validation nlls for the trained models\n",
    "        for k in INN_dict:\n",
    "            print(\"evaluate \",k)\n",
    "\n",
    "            for T_i in T_list_eval:\n",
    "                T_i = round(T_i,5)\n",
    "                \n",
    "\n",
    "                DL_i = validation_data_loader_dict[f\"{T_i}\"]\n",
    "\n",
    "                log_p_theta_val = torch.zeros([0])\n",
    "\n",
    "                for j,(beta_batch,x_batch) in enumerate(DL_i):\n",
    "\n",
    "                    log_p_theta_val_i = INN_dict[k].log_prob(x_batch.to(device),beta_tensor=beta_batch.to(device))\n",
    "                    log_p_theta_val = torch.cat((log_p_theta_val,log_p_theta_val_i.detach().cpu()),0)\n",
    "\n",
    "                nll_dicts[f\"{T_i}\"][k] = log_p_theta_val.detach().cpu()\n",
    "\n",
    "        # Compute the average validation KLs      \n",
    "        average_KL_dicts = {}\n",
    "        error_average_KL_dicts = {}\n",
    "\n",
    "        for k in INN_dict:\n",
    "     \n",
    "            # Combine all the results from the different temperatures to compute the average validation KLD\n",
    "            full_log_p_theta = torch.zeros([0])\n",
    "            full_log_p_gt = torch.zeros([0])\n",
    "\n",
    "            for T_i in T_list_eval:\n",
    "                T_i = round(T_i,5)\n",
    "\n",
    "                full_log_p_theta = torch.cat((full_log_p_theta,nll_dicts[f\"{T_i}\"][k].clone()),0)\n",
    "                full_log_p_gt = torch.cat((full_log_p_gt,log_p_gt_dict[f\"{T_i}\"].clone()),0)\n",
    "\n",
    "                average_KL_T_k,error_average_KL_T_k = get_bootstrap_mean_and_error_KLD(log_p_gt = log_p_gt_dict[f\"{T_i}\"].clone(),log_p_theta = nll_dicts[f\"{T_i}\"][k].clone(),n_bootstrap = n_bootstrap) \n",
    "\n",
    "                KL_per_T_dicts[f\"{T_i}\"][k] = average_KL_T_k\n",
    "                errorKL_per_T_dicts[f\"{T_i}\"][k] = error_average_KL_T_k\n",
    "\n",
    "            assert(full_log_p_theta.shape == full_log_p_gt.shape)\n",
    "\n",
    "            average_KL_k,error_average_KL_k = get_bootstrap_mean_and_error_KLD(log_p_gt = full_log_p_gt,log_p_theta = full_log_p_theta,n_bootstrap = n_bootstrap) \n",
    "\n",
    "            average_KL_dicts[k] = average_KL_k\n",
    "            error_average_KL_dicts[k] = error_average_KL_k\n",
    "\n",
    "    return average_KL_dicts,error_average_KL_dicts,errorKL_per_T_dicts,KL_per_T_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the evaluations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage of learned energy instead of ground truth energy:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"learned_energy\":\n",
    "    mean_KLD_val_dict,error_mean_KLD_val_dict,errorKL_per_T_dicts,KL_per_T_dicts = get_validation_KLs(INN_dict,n_bootstrap = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"learned_energy\":\n",
    "\n",
    "\n",
    "    fs = 20\n",
    "\n",
    "    color_dict = {\n",
    "        \"no_grid_gt_energy\":\"b\",\n",
    "        \"no_grid_learned_energy\":\"r\",\n",
    "        \"grid_gt_energy\": \"b\",\n",
    "        \"grid_learned_energy\": \"r\",\n",
    "        \"volume_preserving\":\"g\"\n",
    "    }\n",
    "\n",
    "    label_dict = {\n",
    "        \"no_grid_gt_energy\":r\"$q^*(x|c)$\",\n",
    "        \"no_grid_learned_energy\":r\"$q^*(x|c) \\approx p_{\\theta}(x|c_0)^{\\frac{c}{c_0}}$\",\n",
    "        \"grid_gt_energy\": r\"$q^*(x|c)$\",\n",
    "        \"grid_learned_energy\": r\"$q^*(x|c) \\approx p_{\\theta}(x|c_0)^{\\frac{c}{c_0}}$\",\n",
    "        \"volume_preserving\":\"volume-preserving flow\"\n",
    "    }\n",
    "\n",
    "    fig,axes = plt.subplots(1,2,figsize = (13,5))\n",
    "\n",
    "    axes[0].set_title(\"TRADE with grid\",fontsize = fs)\n",
    "    axes[1].set_title(\"TRADE without grid\",fontsize = fs)\n",
    "\n",
    "    keys_model = INN_dict.keys()\n",
    "\n",
    "    KLDs = {}\n",
    "    errors_KLD = {}\n",
    "\n",
    "\n",
    "    for i in range(2):\n",
    "        axes[i].get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "        axes[i].set_xlim([0.5,2.0])\n",
    "        \n",
    "        axes[i].set_yscale(\"log\")\n",
    "        axes[i].set_xscale(\"log\")\n",
    "        \n",
    "        axes[i].set_xlabel(r\"$c$\",fontsize = fs)\n",
    "        axes[i].set_ylabel(\"validation KLD\",fontsize = fs)\n",
    "        axes[i].tick_params(axis='both', which='major', labelsize=fs)\n",
    "        axes[i].minorticks_off()\n",
    "        axes[i].set_ylim([3e-3,7e-1])\n",
    "\n",
    "        axes[i].set_xticks([0.5,1,2],[0.5,1.0,2.0])\n",
    "\n",
    "        \n",
    "       \n",
    "\n",
    "\n",
    "    for key in INN_dict.keys():\n",
    "\n",
    "        errors_KLD = []\n",
    "        KLDs = []\n",
    "\n",
    "        for T_i in T_list_eval:\n",
    "            T_i = round(T_i,5)\n",
    "\n",
    "            errors_KLD.append(errorKL_per_T_dicts[f\"{T_i}\"][key])\n",
    "            KLDs.append(KL_per_T_dicts[f\"{T_i}\"][key])\n",
    "\n",
    "\n",
    "        if key.startswith(\"grid\"):\n",
    "            axes[0].errorbar(1 / np.array(T_list_eval),KLDs,yerr = errors_KLD,ls = \"dotted\",marker = \"o\",ms = 3,lw = 2,capsize = 4,label = label_dict[key],color = color_dict[key])\n",
    "\n",
    "        elif key.startswith(\"no_grid\"):\n",
    "            axes[1].errorbar(1 / np.array(T_list_eval),KLDs,yerr = errors_KLD,ls = \"dotted\",marker = \"o\",ms = 3,lw = 2,capsize = 4,label = label_dict[key],color = color_dict[key])\n",
    "        \n",
    "        else:\n",
    "            for i in range(2):\n",
    "                axes[i].errorbar(1 / np.array(T_list_eval),KLDs,yerr = errors_KLD,ls = \"dotted\",marker = \"o\",ms = 3,lw = 2,capsize = 4,label = label_dict[key],color = color_dict[key])\n",
    "\n",
    "    handles, labels = [], []\n",
    "\n",
    "    for handle, label in zip(*axes[0].get_legend_handles_labels()):\n",
    "        handles.append(handle)\n",
    "        labels.append(label)\n",
    "\n",
    "    # Add a single legend below all subplots\n",
    "    fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 0.05), ncol=4,fontsize = fs)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(os.path.join(results_folder,\"ablation_proposal_learned_energy.pdf\"),bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution to sample the evaluation points for the PI loss\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"proposal_distribution_evaluation_points\":\n",
    "    mean_KLD_val_dict,error_mean_KLD_val_dict,errorKL_per_T_dicts,KL_per_T_dicts = get_validation_KLs(INN_dict,n_bootstrap = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"proposal_distribution_evaluation_points\":\n",
    "    fs = 20\n",
    "\n",
    "    color_dict = {\n",
    "        \"grid_model\":\"b\",\n",
    "        \"grid_model_at_base_parameter\":\"r\",\n",
    "        \"grid_training_data\":\"darkorange\",\n",
    "        \"grid_standard_normal\":\"green\",\n",
    "        \"no_grid_model\":\"b\",\n",
    "        \"no_grid_model_at_base_parameter\":\"r\",\n",
    "        \"no_grid_training_data\":\"darkorange\",\n",
    "        \"no_grid_standard_normal\":\"green\"\n",
    "    }\n",
    "\n",
    "    label_dict = {\n",
    "        \"grid_model\":r\"$p_{\\theta}(x|c)$\",\n",
    "        \"grid_model_at_base_parameter\":r\"$p_{\\theta}(x|c_0)$\",\n",
    "        \"grid_training_data\":r\"$p^*(x|c_0)$\",\n",
    "        \"grid_standard_normal\":r\"$\\mathcal{N}(x;0,\\mathbf{1})$\",\n",
    "        \"no_grid_model\":r\"$p_{\\theta}(x|c)$\",\n",
    "        \"no_grid_model_at_base_parameter\":r\"$p_{\\theta}(x|c_0)$\",\n",
    "        \"no_grid_training_data\":r\"$p^*(x|c_0)$\",\n",
    "        \"no_grid_standard_normal\":r\"$\\mathcal{N}(x;0,\\mathbf{1})$\"\n",
    "    }\n",
    "\n",
    "    fig,axes = plt.subplots(1,2,figsize = (13,5))\n",
    "\n",
    "    axes[0].set_title(\"TRADE with grid\",fontsize = fs)\n",
    "    axes[1].set_title(\"TRADE without grid\",fontsize = fs)\n",
    "\n",
    "    keys_model = INN_dict.keys()\n",
    "\n",
    "    KLDs = {}\n",
    "    errors_KLD = {}\n",
    "\n",
    "    for key in INN_dict.keys():\n",
    "\n",
    "        errors_KLD = []\n",
    "        KLDs = []\n",
    "\n",
    "        for T_i in T_list_eval:\n",
    "            T_i = round(T_i,5)\n",
    "\n",
    "            errors_KLD.append(errorKL_per_T_dicts[f\"{T_i}\"][key])\n",
    "            KLDs.append(KL_per_T_dicts[f\"{T_i}\"][key])\n",
    "\n",
    "\n",
    "        if key.startswith(\"grid\"):\n",
    "            ax  = 0\n",
    "        \n",
    "        else:\n",
    "            ax = 1\n",
    "\n",
    "        axes[ax].errorbar(1 / np.array(T_list_eval),KLDs,yerr = errors_KLD,ls = \"dotted\",marker = \"o\",ms = 3,lw = 2,capsize = 4,label = label_dict[key],color = color_dict[key])\n",
    "\n",
    "    handles, labels = [], []\n",
    "\n",
    "    for handle, label in zip(*axes[0].get_legend_handles_labels()):\n",
    "        handles.append(handle)\n",
    "        labels.append(label)\n",
    "\n",
    "    # Add a single legend below all subplots\n",
    "    fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 0.05), ncol=4,fontsize = fs)\n",
    "\n",
    "    for i in range(2):\n",
    "        axes[i].set_yscale(\"log\")\n",
    "        axes[i].set_xscale(\"log\")\n",
    "        axes[i].set_xlabel(r\"$c$\",fontsize = fs)\n",
    "        axes[i].set_ylabel(\"validation KLD\",fontsize = fs)\n",
    "        axes[i].tick_params(axis='both', which='major', labelsize=fs)\n",
    "        axes[i].set_xticks([1/5,0.3,0.5,1,2,3,4,5],[1/5,0.3,0.5,1,2,3,4,5])\n",
    "\n",
    "        axes[i].set_ylim([1e-3,7])\n",
    "\n",
    "        axes[i].hlines(y = 1e-3,xmin = 0.5,xmax = 2.0,color = \"k\",lw = 7)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(os.path.join(results_folder,\"ablation_proposal_dist_eval_points_KL_as_function_of_c.pdf\"),bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of evaluation points vs. number of evaluation parameters (no grid)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"no_grid_number_evaluation_points_vs_evaluation_parms\":\n",
    "    mean_KLD_val_dict,error_mean_KLD_val_dict,_,_ = get_validation_KLs(INN_dict,n_bootstrap = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"no_grid_number_evaluation_points_vs_evaluation_parms\":\n",
    "\n",
    "    mean_KLD_list = []\n",
    "    mean_KLD_error_list = []\n",
    "    is_full_run_list = []\n",
    "\n",
    "    n_samples_per_parameters_list = []\n",
    "    n_evaluation_parameters_list = []\n",
    "\n",
    "    for key in INN_dict:\n",
    "        n_samples_per_parameters_list.append(config_dict[key][\"config_training\"][\"loss_model_params\"][\"n_samples_evaluation_per_param\"])\n",
    "        n_evaluation_parameters_list.append(config_dict[key][\"config_training\"][\"loss_model_params\"][\"n_evaluation_params\"])\n",
    "        mean_KLD_list.append(mean_KLD_val_dict[key])\n",
    "        mean_KLD_error_list.append(error_mean_KLD_val_dict[key])\n",
    "        is_full_run_list.append(full_run(os.path.join(base_path,key)))\n",
    "\n",
    "    n_evaluation_parameters_list = torch.tensor(n_evaluation_parameters_list)\n",
    "    n_samples_per_parameters_list = torch.tensor(n_samples_per_parameters_list)\n",
    "\n",
    "    mean_KLD_list = torch.tensor(mean_KLD_list)\n",
    "    mean_KLD_error_list = torch.tensor(mean_KLD_error_list)\n",
    "    is_full_run_list = torch.tensor(is_full_run_list)\n",
    "\n",
    "    idx = torch.argsort(n_samples_per_parameters_list)\n",
    "\n",
    "    n_samples_per_parameters_list = n_samples_per_parameters_list[idx]\n",
    "    n_evaluation_parameters_list = n_evaluation_parameters_list[idx]\n",
    "    mean_KLD_list = mean_KLD_list[idx]\n",
    "    mean_KLD_error_list = mean_KLD_error_list[idx]\n",
    "    is_full_run_list = is_full_run_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"no_grid_number_evaluation_points_vs_evaluation_parms\":\n",
    "\n",
    "    fs = 20\n",
    "\n",
    "    fig,axes = plt.subplots(1,1,figsize = (13,6))\n",
    "\n",
    "    axes.set_xlabel(r\"$k_B T$\",fontsize = fs)\n",
    "    axes.set_ylabel(\"average validation KLD\",fontsize = fs)\n",
    "\n",
    "    #Plot the runs which ran succesully\n",
    "    axes.errorbar(n_samples_per_parameters_list[is_full_run_list],mean_KLD_list[is_full_run_list],yerr=mean_KLD_error_list[is_full_run_list],ls = \"dotted\",marker = \"o\",ms = 3,lw = 2,capsize = 4,c = \"b\",label = \"full run\")\n",
    "\n",
    "    #Plot the runs which failedy \n",
    "    axes.errorbar(n_samples_per_parameters_list[torch.logical_not(is_full_run_list)],mean_KLD_list[torch.logical_not(is_full_run_list)],yerr=mean_KLD_error_list[torch.logical_not(is_full_run_list)],ls = \"\",marker = \"o\",ms = 3,lw = 2,capsize = 4,c = \"r\",label = \"failed run\")\n",
    "\n",
    "    #axes.errorbar(n_samples_per_parameters_list,mean_KLD_list,yerr=mean_KLD_error_list,ls = \"dotted\",marker = \"o\",ms = 3,lw = 2,capsize = 4,c = \"k\")\n",
    "    axes.set_xscale(\"log\")\n",
    "    axes.tick_params(axis='both', which='major', labelsize=fs)\n",
    "    axes.set_xticks(n_samples_per_parameters_list,[int(n_samples_per_parameters_list[i].item()) for i in range(len(n_samples_per_parameters_list))],rotation=90)\n",
    "    axes.set_xlabel(r\"$n_{points}$\")\n",
    "\n",
    "    ax2 = axes.twiny()\n",
    "    #ax2.errorbar(n_samples_per_parameters_list,mean_KLD_list,yerr=mean_KLD_error_list,ls = \"dotted\",marker = \"o\",ms = 3,lw = 2,capsize = 4,c = \"k\")\n",
    "\n",
    "    #Plot the runs which ran succesully\n",
    "    ax2.errorbar(n_samples_per_parameters_list[is_full_run_list],mean_KLD_list[is_full_run_list],yerr=mean_KLD_error_list[is_full_run_list],ls = \"dotted\",marker = \"o\",ms = 3,lw = 2,capsize = 4,c = \"b\",label = \"full run\")\n",
    "\n",
    "    #Plot the runs which failedy \n",
    "    ax2.errorbar(n_samples_per_parameters_list[torch.logical_not(is_full_run_list)],mean_KLD_list[torch.logical_not(is_full_run_list)],yerr=mean_KLD_error_list[torch.logical_not(is_full_run_list)],ls = \"\",marker = \"o\",ms = 3,lw = 2,capsize = 4,c = \"r\",label = \"failed run\")\n",
    "    ax2.set_xscale(\"log\")\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=fs)\n",
    "    ax2.set_xticks(n_samples_per_parameters_list,[int(n_evaluation_parameters_list[i].item()) for i in range(len(n_evaluation_parameters_list))],rotation=90) # Match ticks\n",
    "    ax2.set_xlabel(r\"$n_{c}$\",fontsize = fs)\n",
    "\n",
    "    handles, labels = [], []\n",
    "\n",
    "    for handle, label in zip(*axes.get_legend_handles_labels()):\n",
    "        handles.append(handle)\n",
    "        labels.append(label)\n",
    "\n",
    "    # Add a single legend below all subplots\n",
    "    fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 0.05), ncol=4,fontsize = fs)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(os.path.join(results_folder,\"ablation_n_evaluation_params_vs_n_evaluation_points_mean_KLD.pdf\"),bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"no_grid_number_evaluation_points_vs_evaluation_parms\":\n",
    "\n",
    "    mean_time = []\n",
    "    std_time = []\n",
    "    n_c = []\n",
    "\n",
    "    for key in INN_dict.keys():\n",
    "\n",
    "       \n",
    "        n_c.append(config_dict[key][\"config_training\"][\"loss_model_params\"][\"n_evaluation_params\"])\n",
    "      \n",
    "        reader_k = SummaryReader(os.path.join(base_path,key),extra_columns=set([\"wall_time\"]))\n",
    "        df_k = reader_k.scalars\n",
    "        df_red = df_k[(df_k[\"tag\"] == \"model_performance/mean_validation_KL\")]\n",
    "       \n",
    "        wall_time_k = df_red[\"wall_time\"].values\n",
    "        y_k = wall_time_k - wall_time_k[0]\n",
    "        x_label = \"wall time [s]\"\n",
    "\n",
    "        #remove the values at which the plotting was performed\n",
    "        dt = torch.tensor(y_k[1:] - y_k[:-1])#[1:]\n",
    "        n = 20\n",
    "        indices = torch.arange(len(dt))  # Get all indices\n",
    "        mask = (indices + 1) % n != 0  # Keep all indices except those divisible by n\n",
    "        dt = dt[mask][1:] / 5\n",
    "\n",
    "        mean_time.append(dt.mean())\n",
    "        std_time.append(dt.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"no_grid_number_evaluation_points_vs_evaluation_parms\":\n",
    "    \n",
    "    # Get the base line for a model with grid based TRADE\n",
    "    path_grid_based_model = os.path.join(home_dir,\"helix/Results/runs_2D_GMM/2025-01-13_ablation_proposal_dist_model_2D_GMM_TRADE_grid/lightning_logs/version_1/\")\n",
    "\n",
    "    reader_k = SummaryReader(path_grid_based_model,extra_columns=set([\"wall_time\"]))\n",
    "    df_k = reader_k.scalars\n",
    "    df_red = df_k[(df_k[\"tag\"] == \"model_performance/mean_validation_KL\")]\n",
    "    \n",
    "    wall_time_k = df_red[\"wall_time\"].values\n",
    "    y_k = wall_time_k - wall_time_k[0]\n",
    "    x_label = \"wall time [s]\"\n",
    "\n",
    "    # Remove the values at which the plotting was performed\n",
    "    dt = torch.tensor(y_k[1:] - y_k[:-1])#[1:]\n",
    "    n = 20\n",
    "    indices = torch.arange(len(dt))  # Get all indices\n",
    "    mask = (indices + 1) % n != 0  # Keep all indices except those divisible by n\n",
    "    dt = dt[mask][1:] / 5\n",
    "\n",
    "    plt.plot(dt)\n",
    "\n",
    "    mean_grid = dt.mean()\n",
    "    std_grid = dt.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"no_grid_number_evaluation_points_vs_evaluation_parms\":\n",
    "     \n",
    "    mean_time = torch.tensor(mean_time)\n",
    "    std_time = torch.tensor(std_time)\n",
    "    n_c = torch.tensor(n_c)\n",
    "\n",
    "    idx = torch.argsort(n_c)\n",
    "\n",
    "    n_c = n_c[idx]\n",
    "    mean_time = mean_time[idx]\n",
    "    std_time = std_time[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"no_grid_number_evaluation_points_vs_evaluation_parms\":\n",
    "\n",
    "    fig,axes = plt.subplots(1,1,figsize = (13,6))\n",
    "\n",
    "    axes.errorbar(n_c,mean_time,yerr=std_time,ls = \"dotted\",marker = \"o\",ms = 3,lw = 2,capsize = 4,c = \"b\",label = \"grid-less TRADE\")\n",
    "    axes.tick_params(axis='both', which='major', labelsize=fs)\n",
    "    axes.minorticks_off()\n",
    "    axes.set_xscale(\"log\")\n",
    "    axes.set_xticks(n_c,[i.item() for i in n_c],rotation=90)\n",
    "    axes.set_ylabel(r\"$t_{epoch}[\\text{s}]$\",fontsize = fs)\n",
    "    axes.set_xlabel(r\"$n_c$\",fontsize = fs)\n",
    "    axes.hlines(y = mean_grid.item(),xmin = n_c.min().item(),xmax = n_c.max().item(),color = \"k\",label = \"grid-based TRADE\")\n",
    "    axes.hlines(y = mean_grid.item()-std_grid.item(),xmin = n_c.min().item(),xmax = n_c.max().item(),color = \"k\",linestyles = \"dashed\")\n",
    "    axes.hlines(y = mean_grid.item()+std_grid.item(),xmin = n_c.min().item(),xmax = n_c.max().item(),color = \"k\",linestyles = \"dashed\")\n",
    "\n",
    "    axes.set_xlim([1.7,220])\n",
    "\n",
    "    handles, labels = [], []\n",
    "\n",
    "    for handle, label in zip(*axes.get_legend_handles_labels()):\n",
    "        handles.append(handle)\n",
    "        labels.append(label)\n",
    "\n",
    "    # Add a single legend below all subplots\n",
    "    fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 0.05), ncol=4,fontsize = fs)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(os.path.join(results_folder,\"ablation_n_evaluation_params_vs_n_evaluation_points_time_per_epoch.pdf\"),bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative weighting loss balancing\n",
    "\n",
    "---\n",
    "\n",
    "TRADE without grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"no_grid_loss_balancing\":\n",
    "    mean_KLD_val_dict,error_mean_KLD_val_dict,_,_ = get_validation_KLs(INN_dict,n_bootstrap = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"no_grid_loss_balancing\":\n",
    "    \n",
    "    mean_KLD_list = []\n",
    "    mean_KLD_error_list = []\n",
    "    fixed_relative_weighting_list = []\n",
    "    is_full_run_list = []\n",
    "\n",
    "    for key in INN_dict:\n",
    "        fixed_relative_weighting_list.append(config_dict[key][\"config_training\"][\"fixed_relative_weighting\"])\n",
    "        mean_KLD_list.append(mean_KLD_val_dict[key])\n",
    "        mean_KLD_error_list.append(error_mean_KLD_val_dict[key])\n",
    "        is_full_run_list.append(full_run(os.path.join(base_path,key)))\n",
    "\n",
    "    fixed_relative_weighting_list = torch.tensor(fixed_relative_weighting_list)\n",
    "    mean_KLD_list = torch.tensor(mean_KLD_list)\n",
    "    mean_KLD_error_list = torch.tensor(mean_KLD_error_list)\n",
    "    is_full_run_list = torch.tensor(is_full_run_list)\n",
    "\n",
    "    idx = torch.argsort(fixed_relative_weighting_list)\n",
    "\n",
    "    fixed_relative_weighting_list = fixed_relative_weighting_list[idx]\n",
    "    mean_KLD_list = mean_KLD_list[idx]\n",
    "    mean_KLD_error_list = mean_KLD_error_list[idx]\n",
    "    is_full_run_list = is_full_run_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"no_grid_loss_balancing\":\n",
    "\n",
    "    for key in INN_dict:\n",
    "        assert(config_dict[key][\"config_training\"][\"data_free_loss_mode\"] == \"PINF_local_Ground_Truth_one_param_V2\")\n",
    "\n",
    "    fs = 20\n",
    "\n",
    "    fig,axes = plt.subplots(1,1,figsize = (13,6))\n",
    "\n",
    "    axes.set_ylabel(\"average validation KLD\",fontsize = fs)\n",
    "\n",
    "    # Plot the runs which ran succesully\n",
    "    axes.errorbar(fixed_relative_weighting_list[is_full_run_list],mean_KLD_list[is_full_run_list],yerr=mean_KLD_error_list[is_full_run_list],ls = \"dotted\",marker = \"o\",ms = 3,lw = 2,capsize = 4,c = \"b\",label = \"full run\")\n",
    "\n",
    "    # Plot the runs which failedy \n",
    "    axes.errorbar(fixed_relative_weighting_list[torch.logical_not(is_full_run_list)],mean_KLD_list[torch.logical_not(is_full_run_list)],yerr=mean_KLD_error_list[torch.logical_not(is_full_run_list)],ls = \"\",marker = \"o\",ms = 3,lw = 2,capsize = 4,c = \"r\",label = \"failed run\")\n",
    "\n",
    "    axes.set_xscale(\"log\")\n",
    "    axes.set_yscale(\"log\")\n",
    "    axes.tick_params(axis='both', which='major', labelsize=fs)\n",
    "\n",
    "    x_ticks = 10.0 **  np.arange(-6,7,1)\n",
    "    axes.set_xticks(x_ticks,[f\"{x_ticks[i].item():.1e}\" for i in range(len(x_ticks))],rotation=90)\n",
    "    axes.set_xlabel(r\"$\\lambda_{\\text{fix}}$\",fontsize = fs)\n",
    "    axes.set_title(\"TRADE without grid\",fontsize = fs)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    print(results_folder)\n",
    "    plt.savefig(os.path.join(results_folder,\"no_grid_ablation_loss_balancing_mean_KLD.pdf\"),bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trade with grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"grid_loss_balancing\":\n",
    "    mean_KLD_val_dict,error_mean_KLD_val_dict,_,_ = get_validation_KLs(INN_dict,n_bootstrap = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"grid_loss_balancing\":\n",
    "    \n",
    "    mean_KLD_list = []\n",
    "    mean_KLD_error_list = []\n",
    "    fixed_relative_weighting_list = []\n",
    "    is_full_run_list = []\n",
    "\n",
    "    for key in INN_dict:\n",
    "        fixed_relative_weighting_list.append(config_dict[key][\"config_training\"][\"fixed_relative_weighting\"])\n",
    "        mean_KLD_list.append(mean_KLD_val_dict[key])\n",
    "        mean_KLD_error_list.append(error_mean_KLD_val_dict[key])\n",
    "        is_full_run_list.append(full_run(os.path.join(base_path,key)))\n",
    "\n",
    "    fixed_relative_weighting_list = torch.tensor(fixed_relative_weighting_list)\n",
    "    mean_KLD_list = torch.tensor(mean_KLD_list)\n",
    "    mean_KLD_error_list = torch.tensor(mean_KLD_error_list)\n",
    "    is_full_run_list = torch.tensor(is_full_run_list)\n",
    "\n",
    "    idx = torch.argsort(fixed_relative_weighting_list)\n",
    "\n",
    "    fixed_relative_weighting_list = fixed_relative_weighting_list[idx]\n",
    "    mean_KLD_list = mean_KLD_list[idx]\n",
    "    mean_KLD_error_list = mean_KLD_error_list[idx]\n",
    "    is_full_run_list = is_full_run_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"grid_loss_balancing\":\n",
    "\n",
    "    for key in INN_dict:\n",
    "        assert(config_dict[key][\"config_training\"][\"data_free_loss_mode\"] == \"PINF_parallel_Ground_Truth_one_param_V2\")\n",
    "\n",
    "    fs = 20\n",
    "\n",
    "    fig,axes = plt.subplots(1,1,figsize = (13,6))\n",
    "\n",
    "    axes.set_ylabel(\"average validation KLD\",fontsize = fs)\n",
    "\n",
    "    # Plot the runs which ran succesully\n",
    "    axes.errorbar(fixed_relative_weighting_list[is_full_run_list],mean_KLD_list[is_full_run_list],yerr=mean_KLD_error_list[is_full_run_list],ls = \"dotted\",marker = \"o\",ms = 3,lw = 2,capsize = 4,c = \"b\",label = \"full run\")\n",
    "\n",
    "    # Plot the runs which failedy \n",
    "    axes.errorbar(fixed_relative_weighting_list[torch.logical_not(is_full_run_list)],mean_KLD_list[torch.logical_not(is_full_run_list)],yerr=mean_KLD_error_list[torch.logical_not(is_full_run_list)],ls = \"\",marker = \"o\",ms = 3,lw = 2,capsize = 4,c = \"r\",label = \"failed run\")\n",
    "\n",
    "    axes.set_xscale(\"log\")\n",
    "    axes.set_yscale(\"log\")\n",
    "    axes.tick_params(axis='both', which='major', labelsize=fs)\n",
    "\n",
    "    x_ticks = 10.0 **  np.arange(-6,7,1)\n",
    "    axes.set_xticks(x_ticks,[f\"{x_ticks[i].item():.1e}\" for i in range(len(x_ticks))],rotation=90)\n",
    "    axes.set_xlabel(r\"$\\lambda_{\\text{fix}}$\",fontsize = fs)\n",
    "    axes.set_title(\"TRADE with grid\",fontsize = fs)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_folder,\"grid_ablation_loss_balancing_mean_KLD.pdf\"),bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of samples used to approximate the expectation values \n",
    "\n",
    "---\n",
    "\n",
    "TRADE without grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"no_grid_number_samples_expectation_approx\":\n",
    "    mean_KLD_val_dict,error_mean_KLD_val_dict,_,_ = get_validation_KLs(INN_dict,n_bootstrap = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"no_grid_number_samples_expectation_approx\":\n",
    "    \n",
    "    mean_KLD_list = []\n",
    "    mean_KLD_error_list = []\n",
    "    n_samples_list = []\n",
    "    is_full_run_list = []\n",
    "\n",
    "    for key in INN_dict:\n",
    "        n_samples_list.append(config_dict[key][\"config_training\"][\"loss_model_params\"][\"n_samples_expectation_approx\"])\n",
    "        mean_KLD_list.append(mean_KLD_val_dict[key])\n",
    "        mean_KLD_error_list.append(error_mean_KLD_val_dict[key])\n",
    "        is_full_run_list.append(full_run(os.path.join(base_path,key)))\n",
    "\n",
    "    n_samples_list = torch.tensor(n_samples_list)\n",
    "    mean_KLD_list = torch.tensor(mean_KLD_list)\n",
    "    mean_KLD_error_list = torch.tensor(mean_KLD_error_list)\n",
    "    is_full_run_list = torch.tensor(is_full_run_list)\n",
    "\n",
    "    idx = torch.argsort(n_samples_list)\n",
    "\n",
    "    n_samples_list = n_samples_list[idx]\n",
    "    mean_KLD_list = mean_KLD_list[idx]\n",
    "    mean_KLD_error_list = mean_KLD_error_list[idx]\n",
    "    is_full_run_list = is_full_run_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"no_grid_number_samples_expectation_approx\":\n",
    "\n",
    "    fs = 20\n",
    "\n",
    "    fig,axes = plt.subplots(1,1,figsize = (13,6))\n",
    "\n",
    "    axes.set_xlabel(r\"$k_B T$\",fontsize = fs)\n",
    "    axes.set_ylabel(\"average validation KLD\",fontsize = fs)\n",
    "\n",
    "    # Plot the runs which ran succesully\n",
    "    axes.errorbar(n_samples_list[is_full_run_list],mean_KLD_list[is_full_run_list],yerr=mean_KLD_error_list[is_full_run_list],ls = \"dotted\",marker = \"o\",ms = 3,lw = 2,capsize = 4,c = \"b\",label = \"full run\")\n",
    "\n",
    "    # Plot the runs which failedy \n",
    "    axes.errorbar(n_samples_list[torch.logical_not(is_full_run_list)],mean_KLD_list[torch.logical_not(is_full_run_list)],yerr=mean_KLD_error_list[torch.logical_not(is_full_run_list)],ls = \"\",marker = \"o\",ms = 3,lw = 2,capsize = 4,c = \"r\",label = \"failed run\")\n",
    "    axes.set_xscale(\"log\")\n",
    "    axes.tick_params(axis='both', which='major', labelsize=fs)\n",
    "    axes.set_xticks(n_samples_list,[int(n_samples_list[i].item()) for i in range(len(n_samples_list))],rotation=90)\n",
    "    axes.set_xlabel(r\"$n_{samples}$\")\n",
    "    axes.set_title(\"TRADE without grid\",fontsize = fs)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_folder,\"no_grid_ablation_n_expectation_approx_mean_KLD.pdf\"),bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"no_grid_number_samples_expectation_approx\":\n",
    "\n",
    "    n_samples = [10000,100,2]\n",
    "    time_stamps = [6]\n",
    "\n",
    "    colors = [\"b\",\"r\",\"orange\"]\n",
    "\n",
    "    fs = 20\n",
    "\n",
    "    fig,axes = plt.subplots(1,1,figsize = (13,6))\n",
    "\n",
    "    gt = np.loadtxt(\"../data/2D_GMM/EX_A_ground_truth.txt\")\n",
    "\n",
    "    for i in range(len(INN_dict.keys())):\n",
    "\n",
    "        key = f\"version_{i}\"\n",
    "\n",
    "        n_samples_k = config_dict[key][\"config_training\"][\"loss_model_params\"][\"n_samples_expectation_approx\"]\n",
    "\n",
    "        if n_samples_k in n_samples:\n",
    "\n",
    "            idx = n_samples.index(n_samples_k)\n",
    "            \n",
    "            # Load the stored causality weights\n",
    "            file_path = os.path.join(base_path,key,\"recorded_data/EX_A.txt\")\n",
    "\n",
    "            data = np.loadtxt(file_path)\n",
    "\n",
    "            for i,t in enumerate(time_stamps):\n",
    "\n",
    "                epoch = int(data[:,t][0] + 1)\n",
    "\n",
    "                axes.plot(data[:,0][1:],-data[:,t][1:],label = r\"$n = $\"+f\"{n_samples_k}\",lw = 3,c = colors[idx])\n",
    "\n",
    "                axes.tick_params(axis='both', which='major', labelsize=fs) \n",
    "                axes.set_xlabel(r\"$c$\",fontsize = fs)\n",
    "                axes.set_ylabel(r\"$\\mathbf{E}\\left[\\frac{\\partial}{\\partial c}\\log q^*(x|c)\\right]$\",fontsize = fs)\n",
    "\n",
    "    axes.plot(gt[:,0],-gt[:,1],label = \"ground truth\",lw = 2,c = \"k\",ls = \"dashed\")\n",
    "\n",
    "    axes.set_title(\"TRADE without grid\",fontsize = fs)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(os.path.join(results_folder,\"ablation_no_grid_number_samples_expectation_approx.pdf\"),bbox_inches='tight')\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRADE with grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"grid_number_samples_expectation_approx\":\n",
    "    mean_KLD_val_dict,error_mean_KLD_val_dict,_,_ = get_validation_KLs(INN_dict,n_bootstrap = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"grid_number_samples_expectation_approx\":\n",
    "    \n",
    "    mean_KLD_list = []\n",
    "    mean_KLD_error_list = []\n",
    "    n_samples_list = []\n",
    "    is_full_run_list = []\n",
    "\n",
    "    for key in INN_dict:\n",
    "        n_samples_list.append(config_dict[key][\"config_training\"][\"loss_model_params\"][\"n_samples_expectation_computation\"])\n",
    "        mean_KLD_list.append(mean_KLD_val_dict[key])\n",
    "        mean_KLD_error_list.append(error_mean_KLD_val_dict[key])\n",
    "        is_full_run_list.append(full_run(os.path.join(base_path,key)))\n",
    "\n",
    "    n_samples_list = torch.tensor(n_samples_list)\n",
    "    mean_KLD_list = torch.tensor(mean_KLD_list)\n",
    "    mean_KLD_error_list = torch.tensor(mean_KLD_error_list)\n",
    "    is_full_run_list = torch.tensor(is_full_run_list)\n",
    "\n",
    "    idx = torch.argsort(n_samples_list)\n",
    "\n",
    "    n_samples_list = n_samples_list[idx]\n",
    "    mean_KLD_list = mean_KLD_list[idx]\n",
    "    mean_KLD_error_list = mean_KLD_error_list[idx]\n",
    "    is_full_run_list = is_full_run_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"grid_number_samples_expectation_approx\":\n",
    "\n",
    "    fs = 20\n",
    "\n",
    "    fig,axes = plt.subplots(1,1,figsize = (13,6))\n",
    "\n",
    "    axes.set_xlabel(r\"$k_B T$\",fontsize = fs)\n",
    "    axes.set_ylabel(\"average validation KLD\",fontsize = fs)\n",
    "\n",
    "    # Plot the runs which ran succesully\n",
    "    axes.errorbar(n_samples_list[is_full_run_list],mean_KLD_list[is_full_run_list],yerr=mean_KLD_error_list[is_full_run_list],ls = \"dotted\",marker = \"o\",ms = 3,lw = 2,capsize = 4,c = \"b\",label = \"full run\")\n",
    "\n",
    "    # Plot the runs which failedy \n",
    "    axes.errorbar(n_samples_list[torch.logical_not(is_full_run_list)],mean_KLD_list[torch.logical_not(is_full_run_list)],yerr=mean_KLD_error_list[torch.logical_not(is_full_run_list)],ls = \"\",marker = \"o\",ms = 3,lw = 2,capsize = 4,c = \"r\",label = \"failed run\")\n",
    "\n",
    "    axes.set_xscale(\"log\")\n",
    "    axes.tick_params(axis='both', which='major', labelsize=fs)\n",
    "    axes.set_xticks(n_samples_list,[int(n_samples_list[i].item()) for i in range(len(n_samples_list))],rotation=90)\n",
    "    axes.set_xlabel(r\"$n_{samples}$\")\n",
    "    axes.set_title(\"TRADE with grid\",fontsize = fs)\n",
    "\n",
    "    handles = []\n",
    "    labels = []\n",
    "\n",
    "    for handle, label in zip(*axes.get_legend_handles_labels()):\n",
    "        handles.append(handle)\n",
    "        labels.append(label)\n",
    "\n",
    "    fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 0.04), ncol=4,fontsize = fs)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(os.path.join(results_folder,\"grid_ablation_n_expectation_approx_mean_KLD.pdf\"),bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"grid_number_samples_expectation_approx\":\n",
    "\n",
    "    n_samples = [10000,100,2]\n",
    "    time_stamps = [6]\n",
    "\n",
    "    colors = [\"b\",\"r\",\"orange\"]\n",
    "\n",
    "    fs = 20\n",
    "\n",
    "    fig,axes = plt.subplots(1,1,figsize = (13,6))\n",
    "\n",
    "    gt = np.loadtxt(\"../data/2D_GMM/EX_A_ground_truth.txt\")\n",
    "\n",
    "    for i in range(len(INN_dict.keys())):\n",
    "\n",
    "        key = f\"version_{i}\"\n",
    "\n",
    "        n_samples_k = config_dict[key][\"config_training\"][\"loss_model_params\"][\"n_samples_expectation_computation\"]\n",
    "\n",
    "        if n_samples_k in n_samples:\n",
    "\n",
    "            idx = n_samples.index(n_samples_k)\n",
    "            \n",
    "            # Load the stored causality weights\n",
    "            file_path = os.path.join(base_path,key,\"recorded_data/EX_A.txt\")\n",
    "\n",
    "            data = np.loadtxt(file_path)\n",
    "\n",
    "            for i,t in enumerate(time_stamps):\n",
    "\n",
    "                epoch = int(data[:,t][0] + 1)\n",
    "\n",
    "                axes.plot(data[:,0][1:],-data[:,t][1:],label = r\"$n = $\"+f\"{n_samples_k}\",lw = 3,c = colors[idx])\n",
    "\n",
    "                axes.tick_params(axis='both', which='major', labelsize=fs) \n",
    "                axes.set_xlabel(r\"$c$\",fontsize = fs)\n",
    "                axes.set_ylabel(r\"$\\mathbf{E}\\left[\\frac{\\partial}{\\partial c}\\log q^*(x|c)\\right]$\",fontsize = fs)\n",
    "\n",
    "    axes.plot(gt[:,0],-gt[:,1],label = \"ground truth\",lw = 2,c = \"k\",ls = \"dashed\")\n",
    "\n",
    "    axes.set_title(\"TRADE with grid\",fontsize = fs)\n",
    "    handles = []\n",
    "    labels = []\n",
    "\n",
    "    for handle, label in zip(*axes.get_legend_handles_labels()):\n",
    "        handles.append(handle)\n",
    "        labels.append(label)\n",
    "\n",
    "    fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 0.04), ncol=4,fontsize = fs)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(os.path.join(results_folder,\"ablation_grid_number_samples_expectation_approx.pdf\"),bbox_inches='tight')       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of grid points (TRADE with grid)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"grid_number_grid_points\":\n",
    "    mean_KLD_val_dict,error_mean_KLD_val_dict,_,_ = get_validation_KLs(INN_dict,n_bootstrap = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"grid_number_grid_points\":    \n",
    "    \n",
    "    mean_KLD_list = []\n",
    "    mean_KLD_error_list = []\n",
    "    n_grid_points = []\n",
    "    is_full_run_list = []\n",
    "\n",
    "    for key in INN_dict:\n",
    "        n_grid_points.append(config_dict[key][\"config_training\"][\"loss_model_params\"][\"n_points_param_grid\"] + 1)\n",
    "        mean_KLD_list.append(mean_KLD_val_dict[key])\n",
    "        mean_KLD_error_list.append(error_mean_KLD_val_dict[key])\n",
    "        is_full_run_list.append(full_run(os.path.join(base_path,key)))\n",
    "\n",
    "    n_grid_points = torch.tensor(n_grid_points)\n",
    "    mean_KLD_list = torch.tensor(mean_KLD_list)\n",
    "    mean_KLD_error_list = torch.tensor(mean_KLD_error_list)\n",
    "    is_full_run_list = torch.tensor(is_full_run_list)\n",
    "\n",
    "    idx = torch.argsort(n_grid_points)\n",
    "\n",
    "    n_grid_points = n_grid_points[idx]\n",
    "    mean_KLD_list = mean_KLD_list[idx]\n",
    "    mean_KLD_error_list = mean_KLD_error_list[idx]\n",
    "    is_full_run_list = is_full_run_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"grid_number_grid_points\":\n",
    "\n",
    "    fs = 20\n",
    "\n",
    "    fig,axes = plt.subplots(1,1,figsize = (13,6))\n",
    "\n",
    "    axes.set_xlabel(r\"$k_B T$\",fontsize = fs)\n",
    "    axes.set_ylabel(\"average validation KLD\",fontsize = fs)\n",
    "\n",
    "    # Plot the runs which ran succesully\n",
    "    axes.errorbar(n_grid_points[is_full_run_list],mean_KLD_list[is_full_run_list],yerr=mean_KLD_error_list[is_full_run_list],ls = \"dotted\",marker = \"o\",ms = 3,lw = 2,capsize = 4,c = \"b\",label = \"full run\")\n",
    "\n",
    "    # Plot the runs which failedy \n",
    "    axes.errorbar(n_grid_points[torch.logical_not(is_full_run_list)],mean_KLD_list[torch.logical_not(is_full_run_list)],yerr=mean_KLD_error_list[torch.logical_not(is_full_run_list)],ls = \"\",marker = \"o\",ms = 3,lw = 2,capsize = 4,c = \"r\",label = \"failed run\")\n",
    "\n",
    "    axes.set_xscale(\"log\")\n",
    "    axes.tick_params(axis='both', which='major', labelsize=fs)\n",
    "    axes.set_xticks(n_grid_points,[int(n_grid_points[i].item()) for i in range(len(n_grid_points))],rotation=90)\n",
    "    axes.set_xlabel(r\"$n_{grid\\;points}$\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_folder,\"ablation_number_grid_points_mean_validation_KLD.pdf\"),bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\epsilon$ for the computation of the causality weights (TRADE with grid)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"grid_epsilon_causality_weight\":\n",
    "    mean_KLD_val_dict,error_mean_KLD_val_dict,errorKL_per_T_dicts,KL_per_T_dicts = get_validation_KLs(INN_dict,n_bootstrap = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"grid_epsilon_causality_weight\":\n",
    "    \n",
    "    mean_KLD_list = []\n",
    "    mean_KLD_error_list = []\n",
    "    epsilon = []\n",
    "    is_full_run_list = []\n",
    "\n",
    "    for key in INN_dict:\n",
    "        epsilon.append(config_dict[key][\"config_training\"][\"loss_model_params\"][\"epsilon_causality_weight\"])\n",
    "        mean_KLD_list.append(mean_KLD_val_dict[key])\n",
    "        mean_KLD_error_list.append(error_mean_KLD_val_dict[key])\n",
    "        is_full_run_list.append(full_run(os.path.join(base_path,key)))\n",
    "\n",
    "    epsilon = torch.tensor(epsilon)\n",
    "    mean_KLD_list = torch.tensor(mean_KLD_list)\n",
    "    mean_KLD_error_list = torch.tensor(mean_KLD_error_list)\n",
    "    is_full_run_list = torch.tensor(is_full_run_list)\n",
    "\n",
    "    idx = torch.argsort(epsilon)[1:]\n",
    "\n",
    "    epsilon = epsilon[idx]\n",
    "    mean_KLD_list = mean_KLD_list[idx]\n",
    "    mean_KLD_error_list = mean_KLD_error_list[idx]\n",
    "    is_full_run_list = is_full_run_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"grid_epsilon_causality_weight\":\n",
    "\n",
    "    fs = 20\n",
    "\n",
    "    fig,axes = plt.subplots(1,1,figsize = (13,6))\n",
    "\n",
    "   \n",
    "    axes.set_ylabel(\"average validation KLD\",fontsize = fs)\n",
    "    axes.set_yscale(\"log\")\n",
    "\n",
    "    # Plot the runs which ran succesully\n",
    "    axes.errorbar(epsilon[is_full_run_list],mean_KLD_list[is_full_run_list],yerr=mean_KLD_error_list[is_full_run_list],ls = \"dotted\",marker = \"o\",ms = 3,lw = 2,capsize = 4,c = \"b\",label = \"full run\")\n",
    "\n",
    "    # Plot the runs which failedy \n",
    "    axes.errorbar(epsilon[torch.logical_not(is_full_run_list)],mean_KLD_list[torch.logical_not(is_full_run_list)],yerr=mean_KLD_error_list[torch.logical_not(is_full_run_list)],ls = \"\",marker = \"o\",ms = 3,lw = 2,capsize = 4,c = \"r\",label = \"failed run\")\n",
    "    \n",
    "    axes.set_xscale(\"log\")\n",
    "    axes.tick_params(axis='both', which='major', labelsize=fs)\n",
    "    axes.set_xticks(epsilon,[f\"{epsilon[i].item():.0e}\" for i in range(len(epsilon))],rotation=90)\n",
    "    axes.set_yticks([5e-3,1e-2,3e-2],[f\"{a:.0e}\" for a in [5e-3,1e-2,3e-2]])\n",
    "    axes.set_xlabel(r\"$\\epsilon$\",fontsize = fs)\n",
    "    axes.minorticks_off()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(os.path.join(results_folder,\"ablation_epsilon_causality_weight_mean_KLD.pdf\"),bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"grid_epsilon_causality_weight\":\n",
    "\n",
    "    epsilons_plot = [1e-5,1e1,1e5]\n",
    "    time_stamps = [1,3,6]\n",
    "    colors = {\n",
    "        \"1e-05\":\"b\",\n",
    "        \"1e+01\":\"r\",\n",
    "        \"1e+05\":\"orange\"\n",
    "        }\n",
    "\n",
    "    fs = 30\n",
    "\n",
    "    fig,axes = plt.subplots(1,3,figsize = (20,7))\n",
    "\n",
    "    for key in INN_dict:\n",
    "        epsilon_k = config_dict[key][\"config_training\"][\"loss_model_params\"][\"epsilon_causality_weight\"]\n",
    "\n",
    "        if epsilon_k in epsilons_plot:\n",
    "            \n",
    "            # Load the stored causality weights\n",
    "            file_path = os.path.join(base_path,key,\"recorded_data/log_causality_weights.txt\")\n",
    "\n",
    "            data = np.loadtxt(file_path)\n",
    "\n",
    "\n",
    "            for i,t in enumerate(time_stamps):\n",
    "\n",
    "                epoch = int(data[:,t][0] + 1)\n",
    "\n",
    "                axes[i].set_title(f\"epoch {epoch}\",fontsize = fs)\n",
    "\n",
    "                p_beta = np.exp(data[:,t][1:])\n",
    "\n",
    "\n",
    "                axes[i].plot(data[:,0][1:],p_beta,label = r\"$\\epsilon = $\"+f\"{epsilon_k:.0e}\",lw = 3,c = colors[f\"{epsilon_k:.0e}\"])\n",
    "\n",
    "                axes[i].tick_params(axis='both', which='major', labelsize=fs) \n",
    "                axes[i].set_xlabel(r\"$c$\",fontsize = fs)\n",
    "                axes[i].set_ylabel(r\"$q_{\\text{grad}}(c)$\",fontsize = fs)\n",
    "\n",
    "    handles = []\n",
    "    labels = []\n",
    "\n",
    "    for handle, label in zip(*axes[1].get_legend_handles_labels()):\n",
    "        handles.append(handle)\n",
    "        labels.append(label)\n",
    "\n",
    "    fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 0.02), ncol=4,fontsize = fs)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(os.path.join(results_folder,\"ablation_epsilon_causality_weight_prob_as_function_of_beta_different_epsilon.pdf\"),bbox_inches='tight')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_MODE == \"grid_epsilon_causality_weight\":\n",
    "\n",
    "    fig,axes = plt.subplots(1,1,figsize = (13,5))\n",
    "    fs = 20\n",
    "\n",
    "    colors = {\n",
    "        \"1e-05\":\"b\",\n",
    "        \"1e+01\":\"r\",\n",
    "        \"1e+05\":\"orange\"\n",
    "        }\n",
    "    \n",
    "    for key in INN_dict:\n",
    "        epsilon_k = config_dict[key][\"config_training\"][\"loss_model_params\"][\"epsilon_causality_weight\"]\n",
    "\n",
    "        if epsilon_k in epsilons_plot:\n",
    "\n",
    "            c_list = []\n",
    "            KLD_list = []\n",
    "            error_list = []\n",
    "\n",
    "            for c in errorKL_per_T_dicts.keys():\n",
    "\n",
    "                c_list.append(1 / float(c))\n",
    "                KLD_list.append(KL_per_T_dicts[c][key])\n",
    "                error_list.append(errorKL_per_T_dicts[c][key])\n",
    "\n",
    "            axes.errorbar(c_list,KLD_list,yerr = error_list,ls = \"dotted\",marker = \"o\",ms = 3,lw = 2,capsize = 4,c = colors[f\"{epsilon_k:.0e}\"],label = r\"$\\epsilon=$\"+f\"{epsilon_k:.0e}\")\n",
    "    \n",
    "    axes.set_xlabel(r\"$c$\",fontsize = fs)\n",
    "    axes.set_ylabel(\"validation KLD\",fontsize = fs)\n",
    "    axes.tick_params(axis='both', which='major', labelsize=fs) \n",
    "    axes.set_yscale(\"log\")\n",
    "    axes.set_xscale(\"log\")\n",
    "    axes.minorticks_off()\n",
    "    axes.set_yticks([1e-3,1e-2,1e-1],[f\"{a:.0e}\" for a in [1e-3,1e-2,1e-1]])\n",
    "    \n",
    "    handles = []\n",
    "    labels = []\n",
    "\n",
    "    for handle, label in zip(*axes.get_legend_handles_labels()):\n",
    "        handles.append(handle)\n",
    "        labels.append(label)\n",
    "\n",
    "    fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, -0.01), ncol=4,fontsize = fs)\n",
    "\n",
    "    plt.savefig(os.path.join(results_folder,\"ablation_epsilon_causality_weight_KLD_as_function_of_c.pdf\"),bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
