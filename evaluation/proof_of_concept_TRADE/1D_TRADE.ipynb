{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm.auto as tqdm\n",
    "from FrEIA.utils import force_to\n",
    "import os\n",
    "from pinf.losses.utils import get_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(7)\n",
    "np.random.seed(7)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "bs_nll = 512\n",
    "n_iter = int(0.5 * 1e5)\n",
    "save_freq = 5000\n",
    "r_final = 0.1\n",
    "gamma_lr_step = r_final ** (1 / n_iter)\n",
    "lamba_weight_decay = 0.0\n",
    "\n",
    "beta_0 = 1.0\n",
    "beta_min = 1 / 3\n",
    "beta_max = 3.0\n",
    "\n",
    "t_burn_in = 0.0\n",
    "t_full = int(0.8 * n_iter)\n",
    "\n",
    "bs_TRADE = 512\n",
    "lambda_TS = 0.1\n",
    "\n",
    "fs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks to model the mean and the standard deviation of a one diemensional normal distribution as a function of $\\beta$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, d_hidden = 128,activation_function = nn.SiLU, device = device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.mean = nn.Sequential(\n",
    "            nn.Linear(1, d_hidden),\n",
    "            activation_function(),\n",
    "            nn.Linear(d_hidden, d_hidden),\n",
    "            activation_function(),\n",
    "            nn.Linear(d_hidden, d_hidden),\n",
    "            activation_function(),\n",
    "            nn.Linear(d_hidden, 1)\n",
    "        )\n",
    "\n",
    "        self.sigma = nn.Sequential(\n",
    "            nn.Linear(1, d_hidden),\n",
    "            activation_function(),\n",
    "            nn.Linear(d_hidden, d_hidden),\n",
    "            activation_function(),\n",
    "            nn.Linear(d_hidden, d_hidden),\n",
    "            activation_function(),\n",
    "            nn.Linear(d_hidden, 1)\n",
    "        )\n",
    "\n",
    "        \n",
    "        for module in self.sigma:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_normal_(module.weight)\n",
    "\n",
    "        for module in self.mean:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_normal_(module.weight)\n",
    "\n",
    "    def forward(self,x,beta_tensor):\n",
    "\n",
    "        assert(x.shape == beta_tensor.shape)\n",
    "\n",
    "        sigma = self.get_sigma(beta_tensor=beta_tensor)\n",
    "        mean = self.get_mean(beta_tensor=beta_tensor)\n",
    "\n",
    "        assert (sigma.shape == x.shape)\n",
    "        assert (mean.shape == x.shape)\n",
    "\n",
    "        log_prob = - (x - mean).pow(2) / (2 * sigma.pow(2)) - 0.5 * torch.log(2 * np.pi * sigma.pow(2))\n",
    "        return log_prob\n",
    "    \n",
    "    def get_sigma(self,beta_tensor):\n",
    "        return self.sigma(beta_tensor.log()).exp()\n",
    "    \n",
    "    def get_mean(self,beta_tensor):\n",
    "        return self.mean(beta_tensor.log()).exp()\n",
    "    \n",
    "    def sample(self,n,beta):\n",
    "        \n",
    "        assert(isinstance(beta,float))\n",
    "\n",
    "        beta_tensor = torch.ones([1,1]).to(self.device) * beta\n",
    "\n",
    "        sigma = self.get_sigma(beta_tensor=beta_tensor).item()\n",
    "        mean = self.get_mean(beta_tensor=beta_tensor).item()\n",
    "\n",
    "        assert(isinstance(sigma,float))\n",
    "        assert(isinstance(mean,float))\n",
    "\n",
    "        x = torch.randn(n).reshape(-1,1) * sigma + mean\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the target distribution\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_target = force_to(torch.distributions.Normal(loc = 0.0,scale = 1.0),device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.to(device)\n",
    "model.train(True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr, weight_decay = lamba_weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer,gamma = gamma_lr_step)\n",
    "\n",
    "folder = \"../../results/TRADE_1D_Proof_of_concept/\"\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_eval = torch.linspace(beta_min,beta_max,1000).to(device)\n",
    "sigma_target = 1 / beta_eval.sqrt().cpu()\n",
    "mu_target = torch.zeros_like(beta_eval).cpu()\n",
    "beta_eval = beta_eval.reshape(-1,1)\n",
    "\n",
    "loss_nll_storage = torch.zeros(n_iter)\n",
    "loss_TRADE_storage = torch.zeros(n_iter)\n",
    "loss_total_storage = torch.zeros(n_iter)\n",
    "\n",
    "for t in tqdm.tqdm(range(n_iter)):\n",
    "    \n",
    "    x_target = p_target.sample([bs_nll]).reshape(-1,1)\n",
    "    beta_0_tensor = torch.ones([bs_nll,1]).to(device) * beta_0\n",
    "\n",
    "    # Get the nll loss\n",
    "    nll = - model(x = x_target,beta_tensor = beta_0_tensor).mean()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        beta_k,left,right = get_beta(\n",
    "            t = t,\n",
    "            t_burn_in=t_burn_in,\n",
    "            t_full=t_full,\n",
    "            beta_star=beta_0,\n",
    "            beta_max=beta_max,\n",
    "            beta_min=beta_min,\n",
    "            mode = \"log-linear\"\n",
    "        )\n",
    "\n",
    "        x_eval = model.sample(n = bs_TRADE,beta = beta_k).to(device)\n",
    "\n",
    "        beta_k_tensor_TRADE = torch.ones((bs_TRADE,1)).to(device) * beta_k\n",
    "        beta_0_tensor_TRADE = torch.ones((bs_TRADE,1)).to(device) * beta_0\n",
    "\n",
    "        d_log_q_d_c = p_target.log_prob(x_eval) / beta_0\n",
    "\n",
    "        # Compute the importance weights\n",
    "        log_q_target_c = beta_k / beta_0 * p_target.log_prob(x_eval) \n",
    "        log_p_model_c = model(x = x_eval,beta_tensor = beta_k_tensor_TRADE)\n",
    "\n",
    "        assert (log_q_target_c.shape == log_p_model_c.shape)\n",
    "\n",
    "        log_omega = (log_q_target_c - log_p_model_c)\n",
    "\n",
    "        assert(log_omega.shape == d_log_q_d_c.shape)\n",
    "\n",
    "        EX = (log_omega.exp() * d_log_q_d_c).mean() / log_omega.exp().mean()\n",
    "\n",
    "        target = (d_log_q_d_c - EX).detach()\n",
    "\n",
    "    beta_k_tensor_TRADE.requires_grad_(True)\n",
    "    d_log_p_theta_d_c = torch.autograd.grad(model(x_eval,beta_k_tensor_TRADE).sum(),beta_k_tensor_TRADE,create_graph=True)[0]\n",
    "    \n",
    "    assert(d_log_p_theta_d_c.shape == target.shape)\n",
    "\n",
    "    loss_TRADE = (target - d_log_p_theta_d_c).pow(2).mean()\n",
    "\n",
    "    loss = nll + lambda_TS * loss_TRADE\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    loss_nll_storage[t] = nll.item()\n",
    "    loss_TRADE_storage[t] = loss_TRADE.item()\n",
    "    loss_total_storage[t] = loss.item()\n",
    "\n",
    "    \n",
    "    #Plot the current state of the networks\n",
    "    if ((t + 1) % save_freq) == 0 or (t == 0):\n",
    "        torch.save(model.state_dict(),os.path.join(folder,f\"step_{t+1}_model.ckpt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training objective\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(3,1, figsize = (10,15))\n",
    "start = 2\n",
    "end = len(loss_TRADE_storage)\n",
    "stepsize = 1\n",
    "n_iter_tensor = torch.arange(0,n_iter)\n",
    "\n",
    "axes[0].plot(n_iter_tensor[start:end:stepsize],loss_nll_storage.detach().numpy()[start:end:stepsize])\n",
    "axes[0].set_title('NLL',fontsize = fs)\n",
    "axes[0].set_xlabel('iteration',fontsize = fs)\n",
    "axes[0].set_ylabel(r\"$\\mathcal{L}_{nll}$\",fontsize = fs)\n",
    "axes[0].tick_params(axis='x', labelsize=fs)\n",
    "axes[0].tick_params(axis='y', labelsize=fs)\n",
    "axes[0].set_xticks(n_iter_tensor[::50000].numpy(),n_iter_tensor[::50000].numpy())\n",
    "\n",
    "axes[1].plot(n_iter_tensor[start:end:stepsize],loss_TRADE_storage.detach()[start:end:stepsize].numpy())\n",
    "axes[1].set_title('TS',fontsize = fs)\n",
    "axes[1].set_xlabel('iteration',fontsize = fs)\n",
    "axes[1].set_ylabel(r\"$\\mathcal{L}_{TS}$\",fontsize = fs)\n",
    "axes[1].tick_params(axis='x', labelsize=fs)\n",
    "axes[1].tick_params(axis='y', labelsize=fs)\n",
    "axes[1].set_xticks(n_iter_tensor[::50000].numpy(),n_iter_tensor[::50000].numpy())\n",
    "\n",
    "axes[2].plot(n_iter_tensor[start:end:stepsize],loss_total_storage.detach().numpy()[start:end:stepsize])\n",
    "axes[2].set_title(r'Objective $\\mathcal{L}= \\mathcal{L}_{nll} + \\lambda \\cdot \\mathcal{L}_{TS}$',fontsize = fs)\n",
    "axes[2].set_xlabel('iteration',fontsize = fs)\n",
    "axes[2].set_ylabel(r\"$\\mathcal{L}$\",fontsize = fs)\n",
    "axes[2].tick_params(axis='x', labelsize=fs)\n",
    "axes[2].tick_params(axis='y', labelsize=fs)\n",
    "axes[2].set_xticks(n_iter_tensor[::50000].numpy(),n_iter_tensor[::50000].numpy())\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the learned parameters to the ground truth\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_pred = model.get_sigma(beta_eval.reshape(-1,1)).detach().flatten().abs().cpu()\n",
    "mu_pred = model.get_mean(beta_eval.reshape(-1,1)).detach().flatten().cpu()\n",
    "\n",
    "\n",
    "fs = 15\n",
    "fig,axes = plt.subplots(2,1,figsize = (10,8))\n",
    "\n",
    "axes[0].set_xlabel(r\"$c$\",fontsize = fs)\n",
    "axes[0].set_ylabel(r\"$\\sigma(c$)\",fontsize = fs)\n",
    "axes[1].set_xlabel(r\"$c$\",fontsize = fs)\n",
    "axes[1].set_ylabel(r\"$\\mu(c$)\",fontsize = fs)\n",
    "\n",
    "axes[0].plot(beta_eval.cpu(),sigma_target,label = \"target\",ls = \"-\",c = \"k\",lw = 3)\n",
    "axes[1].plot(beta_eval.cpu(),mu_target,label = \"target\",ls = \"-\",c = \"k\",lw = 3)\n",
    "\n",
    "axes[0].plot(beta_eval.cpu(),sigma_pred,label = \"prediction\",c = f\"b\",ls = \"-.\",lw = 4)\n",
    "axes[1].plot(beta_eval.cpu(),mu_pred,label = \"prediction\",c = f\"b\",ls = \"-.\",lw = 4)\n",
    "\n",
    "axes[0].tick_params(axis='x', labelsize=fs)\n",
    "axes[0].tick_params(axis='y', labelsize=fs)\n",
    "axes[1].tick_params(axis='x', labelsize=fs)\n",
    "axes[1].tick_params(axis='y', labelsize=fs)\n",
    "\n",
    "axes[0].legend(fontsize = fs)\n",
    "axes[1].legend(fontsize = fs)\n",
    "\n",
    "axes[0].legend(fontsize = fs)\n",
    "axes[1].legend(fontsize = fs)\n",
    "\n",
    "#Markt the position of the training data\n",
    "axes[0].plot([1.0],[1.0],marker = \"o\",ms = 8,c = \"r\")\n",
    "axes[1].plot([1.0],[0.0],marker = \"o\",ms = 8,c = \"r\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(folder,'Learned_parameters_1D_normal.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learned probability density functions and compare them to the ground truth\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_list = [0.2,0.4,0.6,0.8,1.0,2.0,3.0,4.0]\n",
    "x_eval = torch.linspace(-7, 7, 200).reshape(-1,1).to(device)\n",
    "\n",
    "fig,axes = plt.subplots(4,2, figsize = (10,20))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(2):\n",
    "        ax = axes[i][j]\n",
    "    \n",
    "        beta_tensor_i = torch.ones_like(x_eval).to(device) * beta_list[counter]\n",
    "        sigma_ref = 1 / np.sqrt(beta_list[counter])\n",
    "\n",
    "        y_est_i = model(x_eval,beta_tensor_i).exp().detach()\n",
    "        \n",
    "        p_ref = torch.distributions.Normal(0, sigma_ref)\n",
    "        y_target = p_ref.log_prob(x_eval.cpu()).exp().detach().numpy()\n",
    "\n",
    "        ax.plot(x_eval.flatten().cpu(), y_target, label='Target',linewidth = 3)\n",
    "        ax.plot(x_eval.flatten().cpu(), y_est_i.flatten().cpu(), label='Estimated',ls = \"-.\",linewidth = 3)\n",
    "        ax.set_title(r\"$\\beta$\"+f' = {beta_list[counter]}',fontsize = fs)\n",
    "        ax.set_ylim([0,1.0])\n",
    "\n",
    "        ax.set_xlabel('x',fontsize = fs)\n",
    "        ax.set_ylabel(r'$p(x|\\beta)$',fontsize = fs)\n",
    "\n",
    "        ax.legend(fontsize = fs * 0.75) \n",
    "        ax.tick_params(axis='x', labelsize=fs)\n",
    "        ax.tick_params(axis='y', labelsize=fs)\n",
    "\n",
    "        counter += 1\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(folder,'Learned_likelihoods_1D_normal.pdf'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
