{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm.auto as tqdm\n",
    "from FrEIA.utils import force_to\n",
    "from collections import namedtuple\n",
    "from math import sqrt, prod\n",
    "import os\n",
    "import torch\n",
    "from torch.distributions import Distribution\n",
    "from torch.autograd import grad\n",
    "from torch.autograd.forward_ad import dual_level, make_dual, unpack_dual\n",
    "\n",
    "from fff.utils.utils import sum_except_batch\n",
    "from fff.utils.types import Transform\n",
    "\n",
    "from fff.utils.func import (\n",
    "    compute_volume_change,\n",
    "    compute_jacobian\n",
    "    )\n",
    "\n",
    "from pinf.losses.utils import get_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 20\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "train_new = True\n",
    "\n",
    "torch.manual_seed(7)\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "bs_nll = 512\n",
    "bs_TRADE = 512\n",
    "n_iter = int(0.5 * 1e5)\n",
    "plot_freq = 5000\n",
    "r_final = 0.01\n",
    "gamma_lr_step = r_final ** (1 / n_iter)\n",
    "lamba_weight_decay = 0.0\n",
    "\n",
    "c_min = 1 / 3\n",
    "c_max = 3.0\n",
    "\n",
    "t_burn_in = 0.0\n",
    "t_full = int(0.8 * n_iter)\n",
    "\n",
    "beta_recon = 10\n",
    "beta_TRADE = 0.5\n",
    "n_hutchinson_samples_PI = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, d_hidden = 512,activation_function = nn.SiLU, device = device,d_cond = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.encoder_block = nn.Sequential(\n",
    "            nn.Linear(1+d_cond, d_hidden),\n",
    "            activation_function(),\n",
    "            nn.Linear(d_hidden, d_hidden),\n",
    "            activation_function(),\n",
    "            nn.Linear(d_hidden, d_hidden),\n",
    "            activation_function(),\n",
    "            nn.Linear(d_hidden, d_hidden),\n",
    "            activation_function(),\n",
    "            nn.Linear(d_hidden, d_hidden),\n",
    "            activation_function(),\n",
    "            nn.Linear(d_hidden, 1)\n",
    "        )\n",
    "\n",
    "        self.decoder_block = nn.Sequential(\n",
    "            nn.Linear(1+d_cond, d_hidden),\n",
    "            activation_function(),\n",
    "            nn.Linear(d_hidden, d_hidden),\n",
    "            activation_function(),\n",
    "            nn.Linear(d_hidden, d_hidden),\n",
    "            activation_function(),\n",
    "            nn.Linear(d_hidden, d_hidden),\n",
    "            activation_function(),\n",
    "            nn.Linear(d_hidden, d_hidden),\n",
    "            activation_function(),\n",
    "            nn.Linear(d_hidden, 1)\n",
    "        )\n",
    "\n",
    "        \n",
    "        for module in self.encoder_block:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_normal_(module.weight)\n",
    "\n",
    "        for module in self.encoder_block:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_normal_(module.weight)\n",
    "\n",
    "        self.p_0 = force_to(torch.distributions.Normal(loc = 0.0,scale = 1.0),self.device)\n",
    "    \n",
    "    def transform_condition(self,c):\n",
    "        return c.log()\n",
    "\n",
    "    def decode(self,z,c):\n",
    "        x = self.decoder_block(torch.cat((z,self.transform_condition(c)),1)) + z\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def encode(self,x,c):\n",
    "        z = self.encoder_block(torch.cat((x,self.transform_condition(c)),1)) + x\n",
    "\n",
    "        return z\n",
    "\n",
    "    def sample(self,n,c):\n",
    "        \n",
    "        assert(isinstance(c,float))\n",
    "\n",
    "        c_tensor = torch.ones([n,1]).to(self.device) * c\n",
    "\n",
    "        z = self.p_0.sample([n]).reshape(-1,1)\n",
    "        x = self.decode(z = z,c = c_tensor)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def log_prob(self,x,c):\n",
    "\n",
    "        \n",
    "        if not isinstance(c,float):\n",
    "            raise ValueError(\"Beta tensor must be a float\")\n",
    "        \n",
    "        #get the volume change of the transformation\n",
    "        z,jac = compute_jacobian(\n",
    "            x_in = x,\n",
    "            fn = self.encode,\n",
    "            chunk_size=1000,\n",
    "            c = torch.tensor(c).to(self.device).reshape(-1,1)\n",
    "            )\n",
    "\n",
    "        log_jac_det = compute_volume_change(jac).reshape(-1,1)\n",
    "\n",
    "        #get the log-likelihood of the latent code\n",
    "        log_p_z = self.p_0.log_prob(z)\n",
    "        \n",
    "        #print(log_p_z.shape,log_jac_det.shape)\n",
    "        assert log_p_z.shape == log_jac_det.shape\n",
    "        log_p = log_p_z + log_jac_det\n",
    "\n",
    "        return log_p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free Form Flow utils\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The code in this cell is adapted from \n",
    "# Code adapted from the repository 'FFF'\n",
    "# License: MIT License\n",
    "# Source: https://github.com/vislearn/FFF\n",
    "\n",
    "def sample_v(\n",
    "    x: torch.Tensor, hutchinson_samples: int\n",
    ") -> torch.Tensor:\n",
    "    \n",
    "    batch_size, total_dim = x.shape[0], prod(x.shape[1:])\n",
    "\n",
    "    if hutchinson_samples > total_dim:\n",
    "        raise ValueError(\n",
    "            f\"Too many Hutchinson samples: got {hutchinson_samples}, \\\n",
    "                expected <= {total_dim}\"\n",
    "        )\n",
    "\n",
    "    v = torch.randn(\n",
    "        batch_size, total_dim, hutchinson_samples, device=x.device, dtype=x.dtype\n",
    "    )\n",
    "    q = torch.linalg.qr(v).Q.reshape(*x.shape, hutchinson_samples)\n",
    "    return q * sqrt(total_dim)\n",
    "\n",
    "SurrogateOutput = namedtuple(\n",
    "    \"SurrogateOutput\", [\"surrogate\", \"z\", \"x1\", \"regularizations\"]\n",
    ")\n",
    "\n",
    "def volume_change_surrogate(\n",
    "    x: torch.Tensor,\n",
    "    c,\n",
    "    encode: Transform,\n",
    "    decode: Transform,\n",
    "    hutchinson_samples: int = 1\n",
    ") -> SurrogateOutput:\n",
    "    r\"\"\"Computes the surrogate for the volume change term in the change of\n",
    "    variables formula. The surrogate is given by:\n",
    "    $$\n",
    "    v^T f_\\theta'(x) \\texttt{SG}(g_\\phi'(z) v).\n",
    "    $$\n",
    "    The gradient of the surrogate is the gradient of the volume change term.\n",
    "\n",
    "    :param x: Input data. Shape: (batch_size, ...)\n",
    "    :param encode: Encoder function. Takes `x` as input and returns a latent\n",
    "        representation `z` of shape (batch_size, latent_shape).\n",
    "    :param decode: Decoder function. Takes a latent representation `z` as input\n",
    "        and returns a reconstruction `x1`.\n",
    "    :param hutchinson_samples: Number of Hutchinson samples to use for the\n",
    "        volume change estimator. The number of hutchinson samples must be less\n",
    "        than or equal to the total dimension of the data.\n",
    "    :param manifold: Manifold on which the latent space lies. If provided, the\n",
    "        volume change is computed in the tangent space of the manifold.\n",
    "    :return: The computed surrogate of shape (batch_size,), latent representation\n",
    "        `z`, reconstruction `x1` and regularization metrics computed on the fly.\n",
    "    \"\"\"\n",
    "    regularizations = {}\n",
    "    surrogate = 0\n",
    "\n",
    "    x.requires_grad_()\n",
    "    z = encode(x,c)\n",
    "\n",
    "    vs = sample_v(z, hutchinson_samples)\n",
    "\n",
    "    for k in range(hutchinson_samples):\n",
    "        v = vs[..., k]\n",
    "\n",
    "        # $ g'(z) v $ via forward-mode AD\n",
    "        with dual_level():\n",
    "            dual_z = make_dual(z, v)\n",
    "            dual_x1 = decode(dual_z,c)\n",
    "\n",
    "            x1, v1 = unpack_dual(dual_x1)\n",
    "\n",
    "        # $ v^T f'(x) $ via backward-mode AD\n",
    "        (v2,) = grad(z, x, v, create_graph=True)\n",
    "\n",
    "        # $ v^T f'(x) stop_grad(g'(z)) v $\n",
    "        surrogate += sum_except_batch(v2 * v1.detach()) / hutchinson_samples\n",
    "\n",
    "    return SurrogateOutput(surrogate, z, x1, regularizations)\n",
    "\n",
    "def fff_loss(\n",
    "    x: torch.Tensor,\n",
    "    c,\n",
    "    encode: Transform,\n",
    "    decode: Transform,\n",
    "    latent_distribution: Distribution,\n",
    "    hutchinson_samples: int = 1,\n",
    ") -> torch.Tensor:\n",
    "    r\"\"\"Compute the per-sample FFF/FIF loss:\n",
    "    $$\n",
    "    \\mathcal{L} = \\beta ||x - decode(encode(x))||^2 - \\log p_Z(z)\n",
    "        - \\sum_{k=1}^K v_k^T f'(x) stop_grad(g'(z)) v_k\n",
    "    $$\n",
    "    where $E[v_k^T v_k] = 1$, and $ f'(x) $ and $ g'(z) $ are the Jacobians of\n",
    "    `encode` and `decode`.\n",
    "\n",
    "    :param x: Input data. Shape: (batch_size, ...)\n",
    "    :param encode: Encoder function. Takes `x` as input and returns a latent\n",
    "        representation `z` of shape (batch_size, latent_shape).\n",
    "    :param decode: Decoder function. Takes a latent representation `z` as input\n",
    "        and returns a reconstruction `x1`.\n",
    "    :param latent_distribution: Latent distribution of the model.\n",
    "    :param beta: Weight of the mean squared error.\n",
    "    :param hutchinson_samples: Number of Hutchinson samples to use for the\n",
    "        volume change estimator.\n",
    "    :return: Per-sample loss. Shape: (batch_size,)\"\"\"\n",
    "    surrogate = volume_change_surrogate(x,c, encode, decode, hutchinson_samples)\n",
    "    log_prob = latent_distribution.log_prob(surrogate.z)\n",
    "    nll = -sum_except_batch(log_prob) - surrogate.surrogate\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the target distribution\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_1 = force_to(torch.distributions.Normal(loc = 2.0,scale = 0.5),device)\n",
    "p_2 = force_to(torch.distributions.Normal(loc = -2.0,scale = 1.5),device)\n",
    "\n",
    "def p_target(x):\n",
    "    return (p_1.log_prob(x).exp() + p_2.log_prob(x).exp()) / 2\n",
    "\n",
    "def get_training_samples(n):\n",
    "    x_1 = p_1.sample([int(n / 2)])\n",
    "    x_2 = p_2.sample([n - int(n / 2)])\n",
    "\n",
    "    return torch.cat((x_1,x_2),0).reshape(-1,1)\n",
    "\n",
    "c_0 = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.to(device)\n",
    "model.train(True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr, weight_decay = lamba_weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer,gamma = gamma_lr_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_new:\n",
    "    storage_loss_nll = []\n",
    "    storage_loss_TRADE = []\n",
    "    storage_loss_recon = []\n",
    "\n",
    "    for t in tqdm.tqdm(range(n_iter)):\n",
    "\n",
    "        x_target = get_training_samples(bs_nll)\n",
    "        c_0_tensor = torch.ones_like(x_target).to(device) * c_0\n",
    "\n",
    "        loss_nll = fff_loss(\n",
    "            x = x_target,\n",
    "            c = c_0_tensor,\n",
    "            encode=model.encode,\n",
    "            decode=model.decode,\n",
    "            latent_distribution=model.p_0\n",
    "        ).mean()\n",
    "\n",
    "        loss_recon = (x_target - model.decode(model.encode(x_target,c_0_tensor),c_0_tensor)).pow(2).sum(-1)\n",
    "\n",
    "        if beta_TRADE > 0:\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                c_k,left,right = get_beta(\n",
    "                    t = t,\n",
    "                    t_burn_in=t_burn_in,\n",
    "                    t_full=t_full,\n",
    "                    beta_star=c_0,\n",
    "                    beta_max=c_max,\n",
    "                    beta_min=c_min,\n",
    "                    mode = \"log-linear\"\n",
    "                )\n",
    "\n",
    "                x_eval = model.sample(n = bs_TRADE,c = c_k).to(device)\n",
    "\n",
    "                beta_k_tensor_TRADE = torch.ones((bs_TRADE,1)).to(device) * c_k\n",
    "                beta_0_tensor_TRADE = torch.ones((bs_TRADE,1)).to(device) * c_0\n",
    "\n",
    "                d_log_q_d_c = p_target(x_eval).log() / c_0\n",
    "\n",
    "                # Compute the importance weights\n",
    "                log_q_target_c = c_k / c_0 *  p_target(x_eval).log()\n",
    "                log_p_model_c = model.log_prob(x = x_eval,c = float(c_k))\n",
    "\n",
    "                assert (log_q_target_c.shape == log_p_model_c.shape)\n",
    "\n",
    "                log_omega = (log_q_target_c - log_p_model_c)\n",
    "\n",
    "                assert(log_omega.shape == d_log_q_d_c.shape)\n",
    "\n",
    "                EX = (log_omega.exp() * d_log_q_d_c).mean() / log_omega.exp().mean()\n",
    "\n",
    "                target = (d_log_q_d_c - EX).detach()\n",
    "\n",
    "            ####################################\n",
    "\n",
    "            surrogate = 0\n",
    "\n",
    "            c_k_tensor = torch.ones_like(x_eval).to(device) * c_k\n",
    "            c_k_tensor.requires_grad_(True)\n",
    "\n",
    "            vs = sample_v(torch.ones_like(x_eval), n_hutchinson_samples_PI)\n",
    "\n",
    "            for k in range(n_hutchinson_samples_PI):\n",
    "\n",
    "                v = vs[..., k]\n",
    "\n",
    "                ############################################\n",
    "                #compute v^T g'(z,c)\n",
    "                ############################################\n",
    "\n",
    "                c_k_tensor_clone = c_k_tensor.clone().detach().requires_grad_(False)\n",
    "                z_no_c_grad = model.encode(x_eval,c_k_tensor_clone)\n",
    "\n",
    "                (v1,) = grad(model.decode(z_no_c_grad,c_k_tensor_clone), z_no_c_grad, v, create_graph=True,retain_graph=True)\n",
    "\n",
    "                ############################################\n",
    "                #Compute f'(x) v\n",
    "                ############################################\n",
    "\n",
    "                with dual_level():\n",
    "                    dual_x = make_dual(x_eval, v)\n",
    "                    dual_z = model.encode(dual_x,c_k_tensor)\n",
    "\n",
    "                    x1, v2 = unpack_dual(dual_z)\n",
    "\n",
    "                assert(v1.shape == v2.shape)\n",
    "\n",
    "                a = (v1 * v2).sum(-1,keepdim=True)\n",
    "\n",
    "                # Compute the change with respect to the parameter\n",
    "                (v3,) = grad(a.sum(),c_k_tensor,create_graph=True,retain_graph=True)\n",
    "\n",
    "                surrogate += v3 / n_hutchinson_samples_PI\n",
    "\n",
    "            # Compute the PI loss\n",
    "                \n",
    "            log_p_0 = model.p_0.log_prob(model.encode(x_eval,c_k_tensor))\n",
    "\n",
    "            (d_log_p_0_d_c,) = grad(log_p_0.sum(),c_k_tensor, create_graph=True,retain_graph=True)\n",
    "\n",
    "            d_log_p_theta_d_c = d_log_p_0_d_c + surrogate\n",
    "\n",
    "            assert(d_log_p_theta_d_c.shape == target.shape)\n",
    "            \n",
    "            loss_TRADE = (target - d_log_p_theta_d_c).pow(2).mean()\n",
    "\n",
    "\n",
    "            # Get the reconstruction loss\n",
    "            recon_TRADE = (x_eval - model.decode(model.encode(x_eval,c_k_tensor),c_k_tensor)).pow(2).sum(-1)\n",
    "            loss_recon = torch.cat((loss_recon,recon_TRADE),0).mean()\n",
    "\n",
    "            # Combine the different loss contributions\n",
    "            loss = loss_nll + beta_recon * loss_recon + beta_TRADE * loss_TRADE\n",
    "\n",
    "            storage_loss_TRADE.append(loss_TRADE.item())\n",
    "\n",
    "        else:\n",
    "            \n",
    "            loss = loss_nll + beta_recon * loss_recon.mean()\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        storage_loss_nll.append(loss_nll.item())\n",
    "        storage_loss_recon.append(loss_recon.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"../../results/FFF_TRADE_1D_Proof_of_concept\"\n",
    "\n",
    "if train_new:\n",
    "\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    data = np.zeros([len(storage_loss_nll),3])\n",
    "\n",
    "    data[:,0] = storage_loss_nll\n",
    "    data[:,1] = storage_loss_recon\n",
    "    data[:,2] = storage_loss_TRADE\n",
    "\n",
    "    np.savetxt(\n",
    "        fname = os.path.join(folder,\"loss.txt\"),\n",
    "        X = data,\n",
    "        header = \"loss_nll\\tloss_reconstruction\\tloss_TRADE\"\n",
    "    )\n",
    "\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        f = os.path.join(folder,\"final.ckpt\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pretrained model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f = os.path.join(folder,\"final.ckpt\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(3,1,figsize = (10,10))\n",
    "\n",
    "axes[0].set_title(r\"$\\mathcal{L}_{nll}$\")\n",
    "axes[0].plot(storage_loss_nll)\n",
    "axes[0].set_xlabel(\"t [iter.]\")\n",
    "\n",
    "axes[1].set_title(r\"$\\mathcal{L}_{recon}$\")\n",
    "axes[1].plot(storage_loss_recon)\n",
    "axes[1].set_xlabel(\"t [iter.]\")\n",
    "\n",
    "axes[2].set_title(r\"$\\mathcal{L}_{TRADE}$\")\n",
    "axes[2].plot(storage_loss_TRADE)\n",
    "axes[2].set_xlabel(\"t [iter.]\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_eval = [0.5,1.0,2.0]\n",
    "\n",
    "fig,axes = plt.subplots(len(c_eval),1,figsize = (8 ,2.5 * len(c_eval)))\n",
    "\n",
    "x_eval = torch.linspace(-10,6,1000).reshape(-1,1).to(device)\n",
    "p_target_c_0 = p_target(x_eval.squeeze().detach()).squeeze().detach().cpu()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i,c_i in enumerate(c_eval):\n",
    "        axes[i].set_title(f\"c = {c_i}\")\n",
    "\n",
    "        x_i = model.sample(500000,c = c_i).detach().cpu().squeeze()\n",
    "        axes[i].hist(x_i,density=True,bins = 100,edgecolor = \"orange\",histtype = \"step\",linewidth = 2,label = \"model data\")\n",
    "\n",
    "        p_theta = model.log_prob(x = x_eval,c = c_i).detach().cpu().exp()\n",
    "        axes[i].plot(x_eval.squeeze().detach().cpu(),p_theta,lw = 2,c = \"b\",label = \"model density\")\n",
    "\n",
    "        p_target_c = p_target_c_0 ** (c_i / c_0)\n",
    "        Z_i = p_target_c.sum() * (x_eval.squeeze()[1] - x_eval.squeeze()[0])\n",
    "        axes[i].plot(x_eval.squeeze().detach().cpu(),p_target_c.cpu() / Z_i.cpu(),color = \"k\",lw = 2,label = \"target\")\n",
    "        axes[i].set_xlabel(\"x\")\n",
    "        axes[i].set_ylabel(f\"p(x|c = {c_i})\")\n",
    "\n",
    "handles, labels = [], []\n",
    "\n",
    "for handle, label in zip(*axes[len(c_eval)-1].get_legend_handles_labels()):\n",
    "    handles.append(handle)\n",
    "    labels.append(label)\n",
    "\n",
    "# Add a single legend below all subplots\n",
    "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 0.01), ncol=4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(folder,\"densities.pdf\"),bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
